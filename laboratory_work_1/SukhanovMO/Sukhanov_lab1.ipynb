{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1aCo-MixOAsT"
      },
      "source": [
        "## Задачи работы:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s_B_3BDNNvzi"
      },
      "source": [
        "Выполнение лабораторной работы предполагает решение следующих задач:\n",
        "1. Изучить общую схему работы метода обратного распространения ошибки с использованием стохастического градиентного спуска.\n",
        "2. Вывести математические формулы для вычисления градиентов функции ошибки по параметрам нейронной сети и формул коррекции весов.\n",
        "3. Загрузить набор данных MNIST, выполнить предобработку изображений и меток, если это необходимо.\n",
        "4. Реализовать и протестировать метод обратного распространения ошибки для задачи классификации рукописных цифр из набора данных MNIST.\n",
        "\n",
        "Конфигурация нейронной сети:\n",
        "1. Входной слой содержит $w \\times h$ нейронов, что соответствует разрешению одноканального изображения. Для набора MNIST это составляет $28 \\times 28$.\n",
        "2. Выходной слой содержит $k$ нейронов, что соответствует количеству классов изображений Для задачи классификации рукописных цифр MNIST — 10 классов.\n",
        "3. Скрытый слой содержит $s$ нейронов (параметр).\n",
        "4. Параметры метода обучения:\n",
        "   - Скорость обучения (learning rate)\n",
        "   - Размер пачки данных (batch size)\n",
        "   - Количество эпох\n",
        "5. Функции активации:\n",
        "   - На скрытом слое — функция ReLU.\n",
        "   - На выходном слое — функция softmax.\n",
        "   - Входной слой не содержит функции активации.\n",
        "6. Функция ошибки — кросс-энтропия. softmax вместе с кросс-энтропией упрощает вывод формул.\n",
        "7. Контрольные параметры для демонстрации работы нейронной сети:\n",
        "   - Размер пачки данных: от 8 до 64 изображений (в зависимости от доступного объема памяти).\n",
        "   - Скорость обучения: 0.1.\n",
        "   - Количество скрытых нейронов $s$: 300.\n",
        "   - Количество эпох: 20."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f4qWofc08M0u"
      },
      "source": [
        "## Проверка целостности данных:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 888
        },
        "id": "3PPPlMSN8VwM",
        "outputId": "cf37a784-f2df-47b6-c318-94cb28e987cc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Размер обучающей выборки: (60000, 784) (изображений: 60000)\n",
            "Размер тестовой выборки: (10000, 784) (изображений: 10000)\n",
            "Диапазон значений пикселей: [0.00, 1.00]\n",
            "Классы: [0 1 2 3 4 5 6 7 8 9]\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1200x500 with 10 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Визуализация 10 случайных изображений:\n",
            "Размер изображения тренировочного набора: 28 * 28 пикселей\n",
            "Размер изображения тестового набора: 28 * 28 пикселей\n",
            "Уникальные метки: [0 1 2 3 4 5 6 7 8 9]\n",
            "\n",
            "Проверка размерностей:\n",
            "Форма тренировочного датасета изображений: (60000, 784)\n",
            "Форма меток тренировочного датасета: (60000,)\n",
            "Форма тестового набора изображений: (10000, 784)\n",
            "Форма меток тестового набора: (10000,)\n",
            "Количество образцов совпадает: True\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import gzip\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import urllib.request\n",
        "import struct\n",
        "\n",
        "def download_mnist(direc):\n",
        "    os.makedirs(direc, exist_ok=True)\n",
        "    base_url = \"https://raw.githubusercontent.com/fgnt/mnist/master/\"\n",
        "    files = [\n",
        "        'train-images-idx3-ubyte.gz',\n",
        "        'train-labels-idx1-ubyte.gz',\n",
        "        't10k-images-idx3-ubyte.gz',\n",
        "        't10k-labels-idx1-ubyte.gz'\n",
        "    ]\n",
        "    for file in files:\n",
        "        full_path = os.path.join(direc, file)\n",
        "        if not os.path.exists(full_path):\n",
        "            print(f\"Загрузка файла {file}...\")\n",
        "            try:\n",
        "                urllib.request.urlretrieve(base_url + file, full_path)\n",
        "                print(f\"Файл {file} успешно загружен\")\n",
        "            except Exception as e:\n",
        "                raise Exception(f\"Невозможно скачать файл {file} из внешнего источника: {e}\")\n",
        "\n",
        "def load_mnist_images(direc, filename):\n",
        "    full_path = os.path.join(direc, filename)\n",
        "    with gzip.open(full_path, 'rb') as f:\n",
        "        magic, num_images, rows, cols = struct.unpack('>IIII', f.read(16))\n",
        "        data = np.frombuffer(f.read(), np.uint8)\n",
        "    return data.reshape(-1, 784).astype(float) / 255.0, (rows, cols)\n",
        "\n",
        "def load_mnist_labels(direc, filename):\n",
        "    full_path = os.path.join(direc, filename)\n",
        "    with gzip.open(full_path, 'rb') as f:\n",
        "        data = np.frombuffer(f.read(), np.uint8, offset=8)\n",
        "    return data\n",
        "\n",
        "data_dir = 'mnist_data'\n",
        "download_mnist(data_dir)\n",
        "\n",
        "X_train, image_shape_train = load_mnist_images(data_dir, 'train-images-idx3-ubyte.gz')\n",
        "y_train = load_mnist_labels(data_dir, 'train-labels-idx1-ubyte.gz')\n",
        "X_test, image_shape_test = load_mnist_images(data_dir, 't10k-images-idx3-ubyte.gz')\n",
        "y_test = load_mnist_labels(data_dir, 't10k-labels-idx1-ubyte.gz')\n",
        "\n",
        "print(\"\\nРазмер обучающей выборки:\", X_train.shape, f\"(изображений: {X_train.shape[0]})\")\n",
        "print(\"Размер тестовой выборки:\", X_test.shape, f\"(изображений: {X_test.shape[0]})\")\n",
        "print(\"Диапазон значений пикселей:\", f\"[{np.min(X_train):.2f}, {np.max(X_train):.2f}]\")\n",
        "print(\"Классы:\", np.unique(y_train))\n",
        "\n",
        "indices = np.random.choice(len(X_train), 10, replace=False)\n",
        "fig, axs = plt.subplots(2, 5, figsize=(12, 5))\n",
        "for i, idx in enumerate(indices):\n",
        "    row = i // 5\n",
        "    col = i % 5\n",
        "    axs[row, col].imshow(X_train[idx].reshape(28, 28), cmap='gray')\n",
        "    axs[row, col].set_title(f\"Label: {y_train[idx]}\")\n",
        "    axs[row, col].axis('off')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"Визуализация 10 случайных изображений:\")\n",
        "print(f\"Размер изображения тренировочного набора: {image_shape_train[0]} * {image_shape_train[1]} пикселей\")\n",
        "print(f\"Размер изображения тестового набора: {image_shape_test[0]} * {image_shape_test[1]} пикселей\")\n",
        "print(\"Уникальные метки:\", np.unique(y_train))\n",
        "\n",
        "print(\"\\nПроверка размерностей:\")\n",
        "print(\"Форма тренировочного датасета изображений:\", X_train.shape)\n",
        "print(\"Форма меток тренировочного датасета:\", y_train.shape)\n",
        "print(\"Форма тестового набора изображений:\", X_test.shape)\n",
        "print(\"Форма меток тестового набора:\", y_test.shape)\n",
        "print(\"Количество образцов совпадает:\", X_train.shape[0] == len(y_train))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IVzytVL8BhKa"
      },
      "source": [
        "## Выполнение one-hot кодирования:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "yeSPwJBgBolY"
      },
      "outputs": [],
      "source": [
        "def one_hot(y, num_classes=10):\n",
        "    encoded = np.zeros((len(y), num_classes))\n",
        "    for i in range(len(y)):\n",
        "        encoded[i, y[i]] = 1\n",
        "    return encoded\n",
        "\n",
        "Y_train = one_hot(y_train)\n",
        "Y_test = one_hot(y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k5_lpNVeCNV0"
      },
      "source": [
        "## Систематизация знаний:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e_xnawsx9wKv"
      },
      "source": [
        "Графическая интерпретация прямого распространения ошибки в первом слое (размер батча принят равным $32$):\n",
        "\n",
        "$$32\\,\\underset{X\\vphantom{\\Big|}}{\\left\\{ \\vphantom{ \\begin{array}{c}\n",
        "\\rule{40pt}{0.4pt} \\\\[2pt]\n",
        "\\rule{40pt}{0.4pt} \\\\[2pt]\n",
        "\\dots \\\\\n",
        "\\rule{40pt}{0.4pt}\n",
        "\\end{array} } \\right. \\kern -2.4pt \\overbrace{ \\begin{array}{c}\n",
        "\\rule{40pt}{0.4pt} \\\\[2pt]\n",
        "\\rule{40pt}{0.4pt} \\\\[2pt]\n",
        "\\dots \\\\\n",
        "\\rule{40pt}{0.4pt}\n",
        "\\end{array} }^{784}}\n",
        "\\times\n",
        "784\\,\\underset{W1\\vphantom{\\Big|}}{\\left\\{ \\vphantom{ \\begin{array}{ccc}\n",
        "\\rule{0.4pt}{40pt} & \\rule{0.4pt}{40pt} & \\cdots & \\rule{0.4pt}{40pt}\n",
        "\\end{array} } \\right. \\kern -2.4pt \\overbrace{ \\begin{array}{ccc}\n",
        "\\rule{0.4pt}{40pt} & \\rule{0.4pt}{40pt} & \\cdots & \\rule{0.4pt}{40pt}\n",
        "\\end{array} }^{300}}\n",
        "+\n",
        "1\\,\\underset{b1\\vphantom{\\Big|}}{\\left\\{ \\vphantom{\\Big|}\\rule{1pt}{0.4pt} \\right. \\kern -2.4pt \\overbrace{\\vphantom{\\Big|}\\rule{40pt}{0.4pt}}^{300}}\n",
        "=\n",
        "32\\,\\underset{z1\\vphantom{\\Big|}}{\\left\\{ \\vphantom{ \\begin{array}{c}\n",
        "\\rule{40pt}{0.4pt} \\\\[2pt]\n",
        "\\rule{40pt}{0.4pt} \\\\[2pt]\n",
        "\\dots \\\\\n",
        "\\rule{40pt}{0.4pt}\n",
        "\\end{array} } \\right. \\kern -2.4pt \\overbrace{ \\begin{array}{c}\n",
        "\\rule{40pt}{0.4pt} \\\\[2pt]\n",
        "\\rule{40pt}{0.4pt} \\\\[2pt]\n",
        "\\dots \\\\\n",
        "\\rule{40pt}{0.4pt}\n",
        "\\end{array} }^{300}}\n",
        "$$\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;<small>*Сложение матрицы и вектора-строки возможно благодаря механизму Broadcasting в Python.</small>\n",
        "\n",
        "Аналитическая запись взвешенной суммы для $j$-го нейрона скрытого слоя:\n",
        "$$\\,z1_j = \\sum_{i=1}^{784} x_i \\cdot w^{(1)}_{i,j} + b^{(1)}_j, j=1..300$$\n",
        "\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;<small>*Здесь и далее верхний индекс в скобках $(l)$ над весовыми коэффициентами означает их принадлежность слою $l$ нейронной сети.</small>\n",
        "\n",
        "Таким образом, в каждой строке матрицы $z1$ находятся неактивированные взвешеные суммы нейронов скрытого слоя, соответствующие одному примеру (изображению) в батче. После применения функции активации ReLU, не меняющей размерность входной матрицы, получаем матрицу $a1$, которая, как и матрица $z1$, имеет форму $(32, 300)$.\n",
        "Формально эту операцию можно записать следующим образом:\n",
        "$$a1_{j}=relu(z1_{j}), j=1..300$$\n",
        "\n",
        "Графическая интерпретация прямого распространения ошибки во втором слое:\n",
        "$$32\\,\\underset{a1\\vphantom{\\Big|}}{\\left\\{ \\vphantom{ \\begin{array}{c}\n",
        "\\rule{40pt}{0.4pt} \\\\[2pt]\n",
        "\\rule{40pt}{0.4pt} \\\\[2pt]\n",
        "\\dots \\\\\n",
        "\\rule{40pt}{0.4pt}\n",
        "\\end{array} } \\right. \\kern -2.4pt \\overbrace{ \\begin{array}{c}\n",
        "\\rule{40pt}{0.4pt} \\\\[2pt]\n",
        "\\rule{40pt}{0.4pt} \\\\[2pt]\n",
        "\\dots \\\\\n",
        "\\rule{40pt}{0.4pt}\n",
        "\\end{array} }^{300}}\n",
        "\\times\n",
        "300\\,\\underset{W2\\vphantom{\\Big|}}{\\left\\{ \\vphantom{ \\begin{array}{ccc}\n",
        "\\rule{0.4pt}{40pt} & \\rule{0.4pt}{40pt} & \\cdots & \\rule{0.4pt}{40pt}\n",
        "\\end{array} } \\right. \\kern -2.4pt \\overbrace{ \\begin{array}{ccc}\n",
        "\\rule{0.4pt}{40pt} & \\rule{0.4pt}{40pt} & \\cdots & \\rule{0.4pt}{40pt}\n",
        "\\end{array} }^{10}}\n",
        "+\n",
        "1\\,\\underset{b2\\vphantom{\\Big|}}{\\left\\{ \\vphantom{\\Big|}\\rule{1pt}{0.4pt} \\right. \\kern -2.4pt \\overbrace{\\vphantom{\\Big|}\\rule{40pt}{0.4pt}}^{10}}\n",
        "=\n",
        "32\\,\\underset{z2\\vphantom{\\Big|}}{\\left\\{ \\vphantom{ \\begin{array}{c}\n",
        "\\rule{40pt}{0.4pt} \\\\[2pt]\n",
        "\\rule{40pt}{0.4pt} \\\\[2pt]\n",
        "\\dots \\\\\n",
        "\\rule{40pt}{0.4pt}\n",
        "\\end{array} } \\right. \\kern -2.4pt \\overbrace{ \\begin{array}{c}\n",
        "\\rule{40pt}{0.4pt} \\\\[2pt]\n",
        "\\rule{40pt}{0.4pt} \\\\[2pt]\n",
        "\\dots \\\\\n",
        "\\rule{40pt}{0.4pt}\n",
        "\\end{array} }^{10}}\n",
        "$$\n",
        "Аналитическая запись взвешенной суммы для $j$-го нейрона выходного слоя:\n",
        "$$z2_j = \\sum_{i=1}^{300} a1_i \\cdot w^{(2)}_{i,j} + b^{(2)}_j, j=1..10$$\n",
        "\n",
        "Применяя к каждой строке полученной матрицы $z2$ функцию активации softmax, которая также не меняет размерность входа, получаем распределение вероятностей классов для каждого примера (изображения), находящегося в батче:\n",
        "$$a2_{j}=softmax(z2_{j}), j=1..10$$\n",
        "\n",
        "После прямого прохода необходимо вычислить градиенты функции потерь по всем параметрам сети для их последующего обновления. Используем многоклассовую кросс-энтропию в качестве функции потерь:\n",
        "$$E=-\\frac{1}{32}\\sum_{m=1}^{32}\\sum_{j=1}^{10}y_{m,j}\\log(a2_{m,j})$$\n",
        "\n",
        "Здесь $y_{m,j}$ - истинная метка класса для примера $m$ из батча в one-hot кодировке. Функцию потерь можно записать иначе:\n",
        "$$E=\\frac{1}{32}(E_{1 sample}+E_{2 sample}+...+E_{32 sample})$$\n",
        "\n",
        "Для упрощения понимания алгоритма backpropagation рассмотрим нахождение градиента по параметрам второго слоя на примере ошибки $E_{1 sample}$. Понятно, что аналитические выражения градиентов для других примеров в батче будут такими же, как и в первом случае.\n",
        "$$\\frac{\\partial E_{1 sample}}{\\partial w^{(2)}_{i,j}}=\\frac{\\partial E_{1 sample}}{\\partial a2_{j}}\\cdot \\frac{\\partial a2_{j}}{\\partial z2_{j}}\\cdot \\frac{\\partial z2_{j}}{\\partial w^{(2)}_{i,j}} = (a2_{j}-y_{j})\\cdot a1_{i}, j=1..10, i=1..300$$\n",
        "\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;<small>*Благодаря использованию комбинации softmax и кросс-энтропии производная $\\frac{\\partial E_{1 sample}}{\\partial z2_{j}}$ упрощается до разности между выходом нейроной сети и истинным классом входа.</small>\n",
        "\n",
        "Графическая интерпретация для всего батча:\n",
        "$$300\\,\\underset{a1^T\\vphantom{\\Big|}}{\\left\\{ \\vphantom{ \\begin{array}{ccc}\n",
        "\\rule{0.4pt}{40pt} & \\rule{0.4pt}{40pt} & \\cdots & \\rule{0.4pt}{40pt}\n",
        "\\end{array} } \\right. \\kern -2.4pt \\overbrace{ \\begin{array}{ccc}\n",
        "\\rule{0.4pt}{40pt} & \\rule{0.4pt}{40pt} & \\cdots & \\rule{0.4pt}{40pt}\n",
        "\\end{array} }^{32}}\n",
        "\\times\n",
        "32\\,\\underset{\\frac{\\partial E}{\\partial z2} \\vphantom{\\Big|}}{\\left\\{ \\vphantom{ \\begin{array}{c}\n",
        "\\rule{40pt}{0.4pt} \\\\[2pt]\n",
        "\\rule{40pt}{0.4pt} \\\\[2pt]\n",
        "\\dots \\\\\n",
        "\\rule{40pt}{0.4pt}\n",
        "\\end{array} } \\right. \\kern -2.4pt \\overbrace{ \\begin{array}{c}\n",
        "\\rule{40pt}{0.4pt} \\\\[2pt]\n",
        "\\rule{40pt}{0.4pt} \\\\[2pt]\n",
        "\\dots \\\\\n",
        "\\rule{40pt}{0.4pt}\n",
        "\\end{array} }^{10}}\n",
        "=\n",
        "300\\,\\underset{\\frac{\\partial E}{\\partial W2}\\vphantom{\\Big|}}{\\left\\{ \\vphantom{ \\begin{array}{ccc}\n",
        "\\rule{0.4pt}{40pt} & \\rule{0.4pt}{40pt} & \\cdots & \\rule{0.4pt}{40pt}\n",
        "\\end{array} } \\right. \\kern -2.4pt \\overbrace{ \\begin{array}{ccc}\n",
        "\\rule{0.4pt}{40pt} & \\rule{0.4pt}{40pt} & \\cdots & \\rule{0.4pt}{40pt}\n",
        "\\end{array} }^{10}}$$\n",
        "\n",
        "Матричное умножение $a1^T \\times \\frac{\\partial E}{\\partial z2}$ автоматически суммирует вклады всех 32 примеров, а деление (в исходном коде) на размер батча усредняет градиент.\n",
        "\n",
        "Теперь найдем градиент для смещений второго слоя:\n",
        "$$\\frac{\\partial E_{1 sample}}{\\partial b^{(2)}_{j}}=\\frac{\\partial E_{1 sample}}{\\partial a2_{j}}\\cdot \\frac{\\partial a2_{j}}{\\partial z2_{j}}\\cdot \\frac{\\partial z2_{j}}{\\partial b^{(2)}_{j}} = (a2_{j}-y_{j}) \\cdot 1 = a2_{j}-y_{j}, j=1..10$$\n",
        "\n",
        "Для вычисления градиента по всему батчу необходимо найти среднее по стобцам матрицы $\\frac{\\partial E}{\\partial z2}$.\n",
        "\n",
        "По аналогии рассмотрим нахождение градиента по параметрам первого слоя:\n",
        "$$\\frac{\\partial E_{1 sample}}{\\partial w^{(1)}_{i,j}}=\\sum_{k=1}^{10}\\left( \\frac{\\partial E_{1 sample}}{\\partial a2_{k}}\\cdot \\frac{\\partial a2_{k}}{\\partial z2_{k}}\\cdot \\frac{\\partial z2_{k}}{\\partial a1_{j}} \\right)\\cdot \\frac{\\partial a1_{j}}{\\partial z1_{j}}\\cdot \\frac{\\partial z1_{j}}{\\partial w^{(1)}_{i,j}}, i=1..784, j=1..300$$\n",
        "\n",
        "$$\\frac{\\partial E_{1 sample}}{\\partial w^{(1)}_{i,j}}=\\sum_{k=1}^{10}\\left(\\left(a2_{k}-y_{k} \\right)\\cdot w^{(2)}_{j,k}\\right)\\cdot drelu(z1_{j})\\cdot x_{i}$$\n",
        "\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;<small>*Ошибка от выходного слоя распространяется к $a1_{j}$ через 10 разных путей (по одному от каждого выходного нейрона), поэтому мы суммируем все эти вклады.</small>\n",
        "\n",
        "Именно по этой причине в графической интерпретации мы используем умножение на транспонированную матрицу весов $W2$:\n",
        "$$32\\,\\underset{\\frac{\\partial E}{\\partial z2} \\vphantom{\\Big|}}{\\left\\{ \\vphantom{ \\begin{array}{c}\n",
        "\\rule{40pt}{0.4pt} \\\\[2pt]\n",
        "\\rule{40pt}{0.4pt} \\\\[2pt]\n",
        "\\dots \\\\\n",
        "\\rule{40pt}{0.4pt}\n",
        "\\end{array} } \\right. \\kern -2.4pt \\overbrace{ \\begin{array}{c}\n",
        "\\rule{40pt}{0.4pt} \\\\[2pt]\n",
        "\\rule{40pt}{0.4pt} \\\\[2pt]\n",
        "\\dots \\\\\n",
        "\\rule{40pt}{0.4pt}\n",
        "\\end{array} }^{10}}\n",
        "\\times\n",
        "10\\,\\underset{W2^T\\vphantom{\\Big|} \\vphantom{\\Big|}}{\\left\\{ \\vphantom{ \\begin{array}{c}\n",
        "\\rule{40pt}{0.4pt} \\\\[2pt]\n",
        "\\rule{40pt}{0.4pt} \\\\[2pt]\n",
        "\\dots \\\\\n",
        "\\rule{40pt}{0.4pt}\n",
        "\\end{array} } \\right. \\kern -2.4pt \\overbrace{ \\begin{array}{c}\n",
        "\\rule{40pt}{0.4pt} \\\\[2pt]\n",
        "\\rule{40pt}{0.4pt} \\\\[2pt]\n",
        "\\dots \\\\\n",
        "\\rule{40pt}{0.4pt}\n",
        "\\end{array} }^{300}}\n",
        "=\n",
        "32\\,\\underset{\\frac{\\partial E}{\\partial a1}\\vphantom{\\Big|}}{\\left\\{ \\vphantom{ \\begin{array}{c}\n",
        "\\rule{40pt}{0.4pt} \\\\[2pt]\n",
        "\\rule{40pt}{0.4pt} \\\\[2pt]\n",
        "\\dots \\\\\n",
        "\\rule{40pt}{0.4pt}\n",
        "\\end{array} } \\right. \\kern -2.4pt \\overbrace{ \\begin{array}{c}\n",
        "\\rule{40pt}{0.4pt} \\\\[2pt]\n",
        "\\rule{40pt}{0.4pt} \\\\[2pt]\n",
        "\\dots \\\\\n",
        "\\rule{40pt}{0.4pt}\n",
        "\\end{array} }^{300}}$$\n",
        "\n",
        "Затем к каждому элементу матрицы $z1$ применяем функцию активации ReLU и результат поэлементно умножаем на матрицу $\\frac{\\partial E}{\\partial a1}$. Обозначим полученную матрицу как $dz1$. Понятно, что выполнение этих операций не приведет к отличию в пространственных размерах матриц-операндов и матриц-результатов.\n",
        "\n",
        "Для завершения вычисления градиентов по весам первого слоя нужно умножить транспонированный входной батч на матрицу $dz1$:\n",
        "$$784\\,\\underset{X^T\\vphantom{\\Big|}}{\\left\\{ \\vphantom{ \\begin{array}{ccc}\n",
        "\\rule{0.4pt}{40pt} & \\rule{0.4pt}{40pt} & \\cdots & \\rule{0.4pt}{40pt}\n",
        "\\end{array} } \\right. \\kern -2.4pt \\overbrace{ \\begin{array}{ccc}\n",
        "\\rule{0.4pt}{40pt} & \\rule{0.4pt}{40pt} & \\cdots & \\rule{0.4pt}{40pt}\n",
        "\\end{array} }^{32}}\n",
        "\\times\n",
        "32\\,\\underset{dz1 \\vphantom{\\Big|}}{\\left\\{ \\vphantom{ \\begin{array}{c}\n",
        "\\rule{40pt}{0.4pt} \\\\[2pt]\n",
        "\\rule{40pt}{0.4pt} \\\\[2pt]\n",
        "\\dots \\\\\n",
        "\\rule{40pt}{0.4pt}\n",
        "\\end{array} } \\right. \\kern -2.4pt \\overbrace{ \\begin{array}{c}\n",
        "\\rule{40pt}{0.4pt} \\\\[2pt]\n",
        "\\rule{40pt}{0.4pt} \\\\[2pt]\n",
        "\\dots \\\\\n",
        "\\rule{40pt}{0.4pt}\n",
        "\\end{array} }^{300}}\n",
        "=\n",
        "784\\,\\underset{\\frac{\\partial E}{\\partial W1}\\vphantom{\\Big|}}{\\left\\{ \\vphantom{ \\begin{array}{ccc}\n",
        "\\rule{0.4pt}{40pt} & \\rule{0.4pt}{40pt} & \\cdots & \\rule{0.4pt}{40pt}\n",
        "\\end{array} } \\right. \\kern -2.4pt \\overbrace{ \\begin{array}{ccc}\n",
        "\\rule{0.4pt}{40pt} & \\rule{0.4pt}{40pt} & \\cdots & \\rule{0.4pt}{40pt}\n",
        "\\end{array} }^{300}}$$\n",
        "\n",
        "Матричное умножение $X^T \\times dz1$ автоматически суммирует вклады всех 32 примеров, а деление (в исходном коде) на размер батча усредняет градиент.\n",
        "\n",
        "Найдем градиент для смещений первого слоя:\n",
        "$$\\frac{\\partial E_{1 sample}}{\\partial b^{(1)}_{j}}=\\sum_{k=1}^{10}\\left( \\frac{\\partial E_{1 sample}}{\\partial a2_{k}}\\cdot \\frac{\\partial a2_{k}}{\\partial z2_{k}}\\cdot \\frac{\\partial z2_{k}}{\\partial a1_{j}} \\right)\\cdot \\frac{\\partial a1_{j}}{\\partial z1_{j}}\\cdot \\frac{\\partial z1_{j}}{\\partial b^{(1)}_{j}}, j=1..300$$\n",
        "\n",
        "$$\\frac{\\partial E_{1 sample}}{\\partial b^{(1)}_{j}}=\\sum_{k=1}^{10}\\left(\\left(a2_{k}-y_{k} \\right)\\cdot w^{(2)}_{j,k}\\right)\\cdot drelu(z1_{j}) \\cdot 1$$\n",
        "\n",
        "Для вычисления градиента по всему батчу необходимо найти среднее по стобцам матрицы $dz1$.\n",
        "\n",
        "Правило обновления весов для мини-пакетного градиентного спуска:\n",
        "$$\\theta = \\theta - \\frac{\\alpha}{m}\\sum_{i=1}^{m}\\nabla_{\\theta}E(\\theta;x^{(i)};y^{(i)})$$\n",
        "\n",
        "где $x^{(i)}, y^{(i)}$ - входные признаки и метки $i$-го примера в батче, $\\nabla_{\\theta}E(\\theta;x^{(i)};y^{(i)})$ - градиент функции потерь по отношению к параметрам нейронной сети $\\theta$ для $i$-го примера."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ek-xmqhXCq9x"
      },
      "source": [
        "### Приложение:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NqX2R6BM-Zgg"
      },
      "source": [
        "Сначала найдем производную $\\frac{\\partial a2_{i}}{\\partial z2_{j}}$. Softmax имеет следующий вид:\n",
        "$$a2_{i}=\\frac{e^{z2_{i}}}{\\sum_{k=1}^{10}e^{z2_{k}}}$$\n",
        "\n",
        "По правилу дифференцирования дроби $f(x)=\\frac{g(x)}{h(x)}$, имеем $f'(x)=\\frac{g'(x)\\cdot h(x) - g(x)\\cdot h'(x)}{h(x)^2}$. Здесь $g = e^{z2_{i}}$, $h = \\sum_{k=1}^{10}e^{z2_{k}}$.\n",
        "\n",
        "Числитель:\n",
        "$$\\frac{\\partial }{\\partial z2_j}(e^{z2_{i}})=e^{z2_{i}}\\cdot \\frac{\\partial z2_{i}}{\\partial z2_j}$$\n",
        "\n",
        "$$\\frac{\\partial}{\\partial z2_j}(e^{z2_i}) = \\begin{cases}\n",
        "e^{z2_i}, & \\text{если } i = j \\\\\n",
        "0, & \\text{если } i \\neq j\n",
        "\\end{cases}$$\n",
        "\n",
        "Знаменатель:\n",
        "$$\\frac{\\partial }{\\partial z2_j}\\left(\\sum_{k=1}^{10}e^{z2_{k}}\\right)=e^{z2_{j}}$$\n",
        "\n",
        "Рассмотрим два случая: при $j=i$ и $j \\neq i$.\n",
        "\n",
        "$j=i$:\n",
        "\n",
        "$$\\frac{\\partial a2_{i}}{\\partial z2_{j}}=\\frac{e^{z2_{i}} \\cdot \\sum_{k=1}^{10}e^{z2_{k}} - e^{z2_{j}} \\cdot e^{z2_{i}}}{\\left(\\sum_{k=1}^{10}e^{z2_{k}}\\right)^{2}} = \\frac{e^{z2_i}}{\\sum_{k=1}^{10} e^{z2_k}} \\cdot \\frac{\\sum_{k=1}^{10} e^{z2_k} - e^{z2_j}}{\\sum_{k=1}^{10} e^{z2_k}} = a2_i \\cdot (1 - a2_i)$$\n",
        "\n",
        "$j \\neq i$:\n",
        "$$\\frac{\\partial a2_i}{\\partial z2_j} = \\frac{0 \\cdot \\sum_{k=1}^{10} e^{z2_k} - e^{z2_j} \\cdot e^{z2_i}}{\\left(\\sum_{k=1}^{10} e^{z2_k}\\right)^2} = \\frac{-e^{z2_j} \\cdot e^{z2_i}}{\\left(\\sum_{k=1}^{10} e^{z2_k}\\right)^2} = -\\frac{e^{z2_j}}{\\sum_{k=1}^{10} e^{z2_k}} \\cdot \\frac{e^{z2_i}}{\\sum_{k=1}^{10} e^{z2_k}} = -a2_i \\cdot a2_j$$\n",
        "\n",
        "Теперь запишем кросс-энтропию для одного примера:\n",
        "$$E=-\\sum_{k=1}^{10}y_{k}\\log(a2_{k})$$\n",
        "\n",
        "$$\\frac{\\partial E}{\\partial z2_i} = \\frac{\\partial}{\\partial z2_i} \\left[ -\\sum_{k=1}^{10} y_k \\log(a2_k) \\right] = -\\sum_{k=1}^{10} y_k \\frac{\\partial \\log(a2_k)}{\\partial z2_i} = -\\sum_{k=1}^{10} y_k \\cdot \\frac{\\partial \\log(a2_k)}{\\partial a2_k} \\cdot \\frac{\\partial a2_k}{\\partial z2_i} = -\\sum_{k=1}^{10} \\frac{y_k}{a2_k} \\cdot \\frac{\\partial a2_k}{\\partial z2_i}$$\n",
        "\n",
        "Разделим сумму на два случая - когда $i = k$ и когда $i \\neq k$:\n",
        "$$\\frac{\\partial E}{\\partial z2_i} = -\\left[\\frac{y_i}{a2_i} \\cdot \\frac{da2_i}{dz2_i} + \\sum_{k=1, k\\neq i}^{10} \\frac{y_k}{a2_k} \\cdot \\frac{da2_k}{dz2_i}\\right] = -\\frac{y_i}{a2_i} \\cdot a2_i(1 - a2_i) - \\sum_{k=1, k\\neq i}^{10} \\frac{y_k}{a2_k} \\cdot (-a2_k \\cdot a2_i) = $$\n",
        "$$= -y_i + y_i \\cdot a2_i + \\sum_{k=1, k\\neq i}^{10} y_k \\cdot a2_i = a2_i \\cdot (y_i + \\sum_{k=1, k\\neq i}^{10} y_k) - y_i = a2_i \\cdot \\sum_{k=1}^{10} y_k - y_i = $$\n",
        "$$= a2_i \\cdot 1 - y_i = a2_i - y_i \\text{, поскольку } \\sum_{k=1}^{10} y_k = 1$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7_ojCT3_DA_v"
      },
      "source": [
        "## Реализация нейронной сети:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zfrqtgH0NlNL",
        "outputId": "8faec6f1-09ff-44c7-a986-f7980a9cd377"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1: Accuracy 0.9554, Train loss 0.1546, Time 9.34s\n",
            "Epoch 2: Accuracy 0.9727, Train loss 0.0963, Time 9.69s\n",
            "Epoch 3: Accuracy 0.9798, Train loss 0.0729, Time 9.83s\n",
            "Epoch 4: Accuracy 0.9851, Train loss 0.0548, Time 9.64s\n",
            "Epoch 5: Accuracy 0.9873, Train loss 0.0467, Time 10.47s\n",
            "Epoch 6: Accuracy 0.9909, Train loss 0.0369, Time 10.84s\n",
            "Epoch 7: Accuracy 0.9902, Train loss 0.0352, Time 10.24s\n",
            "Epoch 8: Accuracy 0.9936, Train loss 0.0277, Time 15.40s\n",
            "Epoch 9: Accuracy 0.9954, Train loss 0.0217, Time 15.86s\n",
            "Epoch 10: Accuracy 0.9970, Train loss 0.0179, Time 9.76s\n",
            "Epoch 11: Accuracy 0.9979, Train loss 0.0146, Time 9.92s\n",
            "Epoch 12: Accuracy 0.9984, Train loss 0.0124, Time 9.76s\n",
            "Epoch 13: Accuracy 0.9984, Train loss 0.0113, Time 9.68s\n",
            "Epoch 14: Accuracy 0.9987, Train loss 0.0108, Time 9.76s\n",
            "Epoch 15: Accuracy 0.9991, Train loss 0.0087, Time 9.51s\n",
            "Epoch 16: Accuracy 0.9994, Train loss 0.0078, Time 9.76s\n",
            "Epoch 17: Accuracy 0.9995, Train loss 0.0064, Time 9.92s\n",
            "Epoch 18: Accuracy 0.9997, Train loss 0.0060, Time 9.61s\n",
            "Epoch 19: Accuracy 0.9998, Train loss 0.0049, Time 9.72s\n",
            "Epoch 20: Accuracy 0.9999, Train loss 0.0044, Time 10.05s\n",
            "\n",
            "Test accuracy: 0.9829, Test loss: 0.0596\n"
          ]
        }
      ],
      "source": [
        "def init_params(input_size, hidden_size, output_size):\n",
        "    std1 = np.sqrt(2. / input_size)\n",
        "    W1 = np.random.randn(input_size, hidden_size) * std1\n",
        "    b1 = np.zeros(hidden_size)\n",
        "    std2 = np.sqrt(2. / hidden_size)\n",
        "    W2 = np.random.randn(hidden_size, output_size) * std2\n",
        "    b2 = np.zeros(output_size)\n",
        "    return W1, b1, W2, b2\n",
        "\n",
        "def relu(z):\n",
        "    return np.maximum(0, z)\n",
        "\n",
        "def relu_deriv(z):\n",
        "    return (z > 0).astype(float)\n",
        "\n",
        "def softmax(z):\n",
        "    exp_z = np.exp(z)\n",
        "    return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
        "\n",
        "def cross_entropy_loss(y_true, y_pred):\n",
        "    m = y_true.shape[0]\n",
        "    return -np.sum(y_true * np.log(y_pred + 1e-9)) / m\n",
        "\n",
        "def forward(X, W1, b1, W2, b2):\n",
        "    z1 = np.dot(X, W1) + b1\n",
        "    a1 = relu(z1)\n",
        "    z2 = np.dot(a1, W2) + b2\n",
        "    a2 = softmax(z2)\n",
        "    return a2, z1, a1, z2\n",
        "\n",
        "def backprop(X, y_true, a2, z1, a1, W2):\n",
        "    m = X.shape[0]\n",
        "    dz2 = a2 - y_true                 #shape - (m, 10)\n",
        "    dW2 = np.dot(a1.T, dz2) / m       #shape - (300, 10)\n",
        "    db2 = np.sum(dz2, axis=0) / m     #shape - (1, 10)\n",
        "    da1 = np.dot(dz2, W2.T)           #shape - (m, 300)\n",
        "    dz1 = da1 * relu_deriv(z1)        #shape - (m, 300)\n",
        "    dW1 = np.dot(X.T, dz1) / m        #shape - (784, 300)\n",
        "    db1 = np.sum(dz1, axis=0) / m     #shape - (1, 300)\n",
        "    return dW1, db1, dW2, db2\n",
        "\n",
        "def update(W1, b1, W2, b2, dW1, db1, dW2, db2, lr):\n",
        "    W1 -= lr * dW1\n",
        "    b1 -= lr * db1\n",
        "    W2 -= lr * dW2\n",
        "    b2 -= lr * db2\n",
        "    return W1, b1, W2, b2\n",
        "\n",
        "def accuracy(X, y, W1, b1, W2, b2):\n",
        "    a2, _, _, _ = forward(X, W1, b1, W2, b2)\n",
        "    preds = np.argmax(a2, axis=1)\n",
        "    acc = np.mean(preds == y)\n",
        "    return acc\n",
        "\n",
        "def train(W1, b1, W2, b2, X, Y, y_labels, batch_size, epochs, lr):\n",
        "    n = X.shape[0]\n",
        "    for epoch in range(epochs):\n",
        "        start = time.time()\n",
        "        perm = np.random.permutation(n)\n",
        "        X_shuf = X[perm]\n",
        "        Y_shuf = Y[perm]\n",
        "        for i in range(0, n, batch_size):\n",
        "            end = min(i + batch_size, n)\n",
        "            Xb = X_shuf[i:end]\n",
        "            Yb = Y_shuf[i:end]\n",
        "            a2, z1, a1, z2 = forward(Xb, W1, b1, W2, b2)\n",
        "            dW1, db1, dW2, db2 = backprop(Xb, Yb, a2, z1, a1, W2)\n",
        "            W1, b1, W2, b2 = update(W1, b1, W2, b2, dW1, db1, dW2, db2, lr)\n",
        "        a2_train, _, _, _ = forward(X, W1, b1, W2, b2)\n",
        "        train_acc = accuracy(X, y_labels, W1, b1, W2, b2)\n",
        "        train_loss = cross_entropy_loss(Y, a2_train)\n",
        "        elapsed = time.time() - start\n",
        "        print(f\"Epoch {epoch + 1}: Accuracy {train_acc:.4f}, Train loss {train_loss:.4f}, Time {elapsed:.2f}s\")\n",
        "    return W1, b1, W2, b2\n",
        "\n",
        "input_size = 784\n",
        "hidden_size = 300\n",
        "output_size = 10\n",
        "lr = 0.1\n",
        "batch_size = 32\n",
        "epochs = 20\n",
        "\n",
        "W1, b1, W2, b2 = init_params(input_size, hidden_size, output_size)\n",
        "W1, b1, W2, b2 = train(W1, b1, W2, b2, X_train, Y_train, y_train, batch_size, epochs, lr)\n",
        "\n",
        "a2_test, _, _, _ = forward(X_test, W1, b1, W2, b2)\n",
        "test_acc = accuracy(X_test, y_test, W1, b1, W2, b2)\n",
        "test_loss = cross_entropy_loss(Y_test, a2_test)\n",
        "print(f\"\\nTest accuracy: {test_acc:.4f}, Test loss: {test_loss:.4f}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
